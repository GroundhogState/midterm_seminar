# Midterm notes



## Intro

 5:20 (5:20)

Helium is the second-most abundant element in the known universe, comprising 23 percent of baryone mass, most of which produced in nucleosynthesis, three minutes after the big bang. Helium is a major product of stellar fusion, and takes its name from the greek sun titan Helios as it was first detected by observation of a 587nm line during a solar eclipse which could not be identified with any other known elements. Back on earth, Helium comprises only 5 of the atmosphere, as it is light enough to float through the stratosphere and escape into space. The founder of our school, Mark Oliphant, was part of the team that first identified Helium-3 by mass-to-charge ratio separation from the tritium nucleus. In fact, Helium has two stable isotopes, the fermionic helium 3 and the bosonic helium 4. In the furnace of the sun, spin certainly has thermodynamic consequences, but on the other end of the temperature scale, near zero kelvin, the fun really starts. Helium-4, a boson, is not bound by the exclusion principle and so many bosons can occupy the same quantum state. When a large fraction of a bosonic gas condenses into the ground state of the confining potential, the resulting phase of matter is called a Bose-Einstein condensate. This image depicts the process of condensation, with a notable population of the zero-momentum mode coexisting with a thermal background at a few hundred microkelvin, eventually subsuming most of the gas at ultralow temperatures. Bose-Einstein condensates, or BEC, are named for Satyendra Nath Bose and Albert Einstein, both credited for their prediction some 70 years before the state was produced in a lab.

The first elements condensed were the alkali atoms Sodium and Potassium, and now a whole zoo of elements have been cooled to degeneracy, both bosons and fermions, all with their advantages and drawbacks. Helium is presently the only noble gas to have been condensed, and it's likely to remain that way. Since the first BEC in 1995, the field has flourished into a global industry, and we'll take a peek at elements of the cutting edge along the way today.

Of course, we're here in Canberra, where I've been working in the Helium BEC group for the last three years. We're a very collaborative group, with many hands on each project, so I'll tell you about the works in which I've been substantially involved. 
But sure, you may say, BECs are pretty cool, but what are they actually good for? I'll give you three reasons now and expand upon them shortly.
First, Given their exquisite isolation, they provide pristine conditions to realize ideal conditions for tests of fundamental theories.
Second, BECs let us routinely distil extensive coherence in large systems, clearing paths into the study of macroscopic coherence as in superfluids and superconductors.
Third, the precision engineering of many-body quantum states requires vanishing entropy in the initial state, readily found near absolute zero. 

So now you want a BEC, let's find talk about how to make one. 

Here's an anatomical diagram of our BEC machine. We feed garden-variety Helium-4 in at room temperature, through a liquid-nitrogen cooled plasma discharge chamber, and through a skimming nozzle to produce a Helium beam with mean velocity about 700 metres per second. We use laser cooling to collimate and slow the beam to 70 metres per second, loading a magneto-optical trap at at the end of the beamline. We use a faint laser to nudge a beam of cold Helium through another differential pumping stage at the back of the MOT, loading a second MOT in the science chamber at about 1 millikelvin. Here, the background pressure is fifteen orders of magnitude smaller than atmospheric pressure. We use Doppler cooling again in one dimension to reduce the temperature to about 200 microkelvin and polarizing the atoms for transfer into a magnetic trap. The biplanar quadrupole Ioffe configuration, or BiQUIC trap, generates both the quadrupole field for the MOT, a uniform field for the Doppler cooling stage, and a harmonic magnetic trap, by adding a controllable DC bias to a quadrupole field. The atoms are subject to a magnetic potential because of the coupling of their magnetic moment to the field. Because the metastable state has three magnetic sublevels, with magnetic quantum numbers 1, 0, and -1, only the +1 atoms seek the minimum at the centre of the magnetic trap. Once the trap is loaded, radio-forced evaporative cooling reduces the temperature below the critical point, crossing the Bose-Einstein condensation threshold with a million atoms at a few hundred nanokelvin. 

Helium wields a double-edged sword - a 20eV gap separation between its ground and lowest excited state. This makes it inaccessible to modern lasers, but is redeemed by the two-hour lifetime of the metastable 2^3S_1 state, which is doubly forbidden to decay to the ground state, and opens up an opportune laser cooling transition at the micron scale. This gap is also troublesome because it leads to Penning ionization, where collisions between atoms release this energy and ionize one or both of the colliding pair, but in general they blast right out of whatever trap you're using, and this leads to unacceptable loss rates. This can be mitigated by applying a strong polarizing magnetic field, where the ionization process no longer conserves angular momentum and is suppressed. This suppression is less effective for heavier noble gases, which makes them unattractive candidates for condensation.

But this gap is also what makes Helium worth working with, allowing us to employ detectors typically used in much higher energy experiments. About 350ms of freefall under our trap is our multichannel plate - delay line detector. The first stage is an array of electron-multipliers with 100 micron resolution. If a helium atom collides with the surface of a pore, the 20eV gap liberates a handful of electrons from the skin of the pore, which are accelerated by a 2kV potential down an electron-multiplying channel, amplifying single-atom incidences by a factor of about a million. The electrons cascade across the delay-line, creating pulses of current whose relative travel time to the ends of the line determine the position of the event in one dimension.  A software correlator turns pulse arrival times from two perpendicular delay-line windings into time and position measurements, giving us full 3D sampling of the far-field momentum distribution with single-atom sensitivity, unavailable to any other condensed species. 

So now you've got a BEC. What do you want to do with it?


## Metrology I: Spectroscopy

6:17 (11:38)

How about some good old-fashioned spectroscopy?

Arguably, spectroscopy is the mother of all our understanding of matter. From  spectroscopy was born quantum theory, spin, and the prediction of antimatter in relativistic quantum electrodynamics. But for all its triumphs, our best physical theory, quantum electrodynamics, falls short in some high-precision instances. For example, if you switch the electron in Hydrogen for a Muon and measure the respective Lamb shifts, you can determine the radius of the proton and find that it's different in each case. We need more measurements to constrain or discard competing theories. Fortunately, the simplicity of Helium allows predictions of its transition lines to some parts per trillion, accurate enough to compete with modern spectroscopy. 

So of all the lines in Helium, which are the most informative to measure? Being diligent scientists, we take reproduction of measurements seriously, and so existing points of difference between theory and data are obvious touchpoints - and this guides us to the 2L-5D transitions, which differ at the scale of a few MHz. Precision measurements of the spin-flipping 2^3S_1 to 2^1S_0 transition at 1557nm also disagree by a few MHz, which bears further investigation. More egregious is that the NIST database lists measurements of transitions from the 2^3P to 5^3D and 5^3S states that differ from theory by about thirteen gigahertz, which absolutely warrants re-examination. Another draw to these transitions is that theoretical uncertainty in the predicted energy levels is reduced at higher energy because the electrons are less subject to relativistic effects. Conversely, low-lying energy levels are therefore of interest precisely because precise measurements provide more useful information. 

We can kill all of these birds with a single stone if we can apply a probe beam, in blue, to the upper state of our cooling transition, in red. Fortunately, there happens to be a perfect window of opportunity in our standard BEC production sequence, so we're going to need to dive into some detail.

As I said, an essential part of our BEC production is an optical cooling and spin-polarizing stage which precedes the loading of our magnetic trap. This ensures a nice large atom number and low temperature. This give us a nice big phase space density, a dimensionless number which compares the length scale of quantum interference, the de Broglie wavelength, with the interparticle spacing given by the particle density n. So, disturbances to this initial condition by atom loss or heating will reduce the phase space density input to the evaporation stage. The Bose-Einstein condensation threshold occurs when the phase space density crosses about 2 - for comparison, atmosphere has a phase space density about one ten millionth of that. One can therefore think of the RF evaporation as a phase space amplifier in the following way:

Evaporative cooling works by creating a resonance between trapped and untrapped magnetic states of atoms with a specific Zeeman splitting by exposing the cloud to radio frequency radiation. Energetic atoms travel up the magnetic field gradient, shown by the thickness of the purple lines, to the ellipsoidal shell defined by a fixed field strength at which the atoms resonate with the radio waves. The atoms are then transferred into free states and leave the trap, taking with an amount of energy greater than the ensemble average. This basically cuts out the upper tail of the Maxwell-Boltzmann distribution, driving down the atom number, but also the temperature once the cloud rethermalizes. If you get this right, you continuously increase the phase space density by making the cloud cooler and smaller, until you get to BEC. The endpoint of the RF chirp fixes an upper bound to the energy of the trapped atoms, and hence a temperature. Then the phase space density can be estimated by counting the number of atoms you have left. We measure this atom number using Helium's unique detectability - by applying broadband radio pulses to the trap we free about 0.5% of the atoms at a time, creating a series of pulses of coherent matter waves, known as an atom laser. This resolves on our detector as a series of discrete particle detection events, which we sum up with an abacus.
So by controlling our independent variable, the applied laser frequency, we have a gain mechanism that allows us to measure the dependent variable, which is the phase space density reduction by resonant scattering of photons from the probe beam. 

Now, getting the probe beam to hit the atoms is another story altogether.

The light source for our probe beam is a super-cool tunable laser which was generously loaned to us by Chris Vale's group at Swinburne uni, which is actually N lasers in series. First, a 1064nm seed is doubled by second-harmonic generation to 532nm, which is used as the pump beam for a tunable titanium-sapphire laser with a variable output that we operate within the 800nm range. This is doubled again by a nonlinear crystal to around 400nm. A fraction of the light is fed into the two ports of a High Finesse WS8 wavemeter, which we periodically calibrate with respect to a two-photon transition in a Cesium cell. This gives us a monitor for our software lock which is stable to 100kHz. We use the first diffracted order of an AOM to regulate the intensity of the light with respect to a photodiode placed after the output of the fibre that couples the laser generation table to the vacuum entry window. The profile and polarization of the beam are controlled with waveplates and lenses after a polarizing beam splitter. A translation mount on the final lens gives us micrometer precision in the placement of the focal point on the beam, which we use to align the beam on the trap, threading a ten micron needle in the dark 20 micro-radian accuracy.

So, results! This is a pair of resonance peaks I observed by applying the probe beam at two different magnetic field strengths, and calibrated independently with radio spectroscopy. 

On the horizontal axis is the probe laser frequency, relative to the zero-field prediction. The vertical axis is the response as measured by atom number loss relative to a model obtained by interpolating between calibration shots adjacent to each data shot. Theory values, in orange, are Zeeman shifted from the zero-field value according to the field calibration, with uncertainty included. The blue curve is the weaker magnetic field, and the red is the stronger, at eleven and eighteen Gauss, respectively. Using two field values gives us better statistics in calculating the field-free resonance, and also helps verify our identification of the transitions. This is the first recording of this transition, and it's super cool because it's actually a forbidden spin-flip transition, four orders of magnitude weaker than our cooling transition. This line nails two points of contention - the 2L-5D interval and the singlet-triplet gap. 

For completeness, these are all the transitions we measured. Note that in the singlet-triplet case the beam was focuses, but in the rest the beam was collimated and much larger than the BEC, so we demonstrate a sensitivity to over four orders of magnitude of oscillator strengths, limited essentially by laser power. Theory values aren't shown in this plot, because unfortunately at the field strengths we chose, the D2 and D3 levels mix thanks to some avoided crossings, and I haven't done the math yet.

The statistical error in the centre frequency of our Lorentzian fits is less than a MHz, fixing these transitions to some parts per billion, with MHz differences from theory.  The last measurement of the 2^3P-5^3D gap could not resolve the fine structure splitting, about 300MHz, but we resolve them with excellent visibility. To give credit where credit is due, the last measurement of these transitions used a discharge cell submerged in liquid nitrogen, passed through a window to an in-vacuum diffraction grating and illuminating a phosphor screen with lines whose splitting was measured with a ruler against a Mercury reference. Still, they were wrong.

But, our ultimate accuracy is limited by the wavemeter to 20MHz in the worst case, or 4MHz when close enough to the calibration line. Unfortunately, we don't have the precision required to compete with the state of the art, but had we a frequency comb reference then we'd be in the game for tests of fundamental physics. Still, we correct the previous values by thirteen gigahertz,  update four values in the NIST database, and add a new line to boot. We conclude that within our experimental uncertainty, QED is correct.


## Metrology II: Tuneout

But I promised you tests of QED, so let's recap the other piece of spectroscopy I've been working on with Kieran and Bryce.

Tuneout wavelengths are those for which an atom is completely ignorant of the AC electric field. As a brief refresher, the level spacing of a two-level atom expands when exposed to an oscillating field which is red relative to the natural splitting. Conversely, a blue-detuned light field compresses the level splitting. Atoms are subject to a potential proportional to the electric field intensity, where the constant of proportionality is the frequency-dependent polarizability. In the red-detuned case atoms are attracted to intensity maxima, and minima in the blue-detuned case. This is the dipole force, the basis of the optical tweezer. The polarizability of a multi-level atom therefore is dependent on many simultaneous level shifts, and the tuneout wavelengths for a given state are those at which all of these shifts perfectly cancel, resulting in a zero polarizability and rendering the atoms completely blind to radiation at that frequency. Thanks to Helium's structural simplicity, these wavelengths can be predicted with absurd accuracy, including high-order corrections for effects of QED. Therefore a measurement of a Tuneout in Helium provides a stringent, absolute constraint.

It's been said that frequency begets the finest measurement, so as budding metrologists we used a second new spectroscopic technique to measure the tuneout wavelength. A BEC in our magnetic trap oscillates at the natural frequency of the trap. This gives us a reference clock that we beat against an oscillating BEC in a combined laser and magnetic trap. The restoring force in the perturbed trap depends on the polarizability, and so the change in the square of the trap frequency is, to first order, proportional to polarizability. The polarizability is negative to the red of the tuneout, forming a repulsive potential that decreases the spring constant and hence the trapping frequency. Conversely, the polarizability is positive to the blue of the tuneout, creating an attractive potential that increases the spring constant and the oscillation frequency. 

>> UNITS ON GRAPH
We give the BEC a kick with a magnetic field to start the oscillation, and then measure the impulse response function by outcoupling, again with an atom laser, and recover the in-trap oscillation frequency by a sinusoidal fit to the BEC displacement on our detector. We measure the change in the squared trap frequency as a function of applied laser wavelength, and interpolate to find the Tuneout wavelength at the zero crossing.

With zero-crossing uncertainties on the order of 100MHz, we are able to resolve the dependence of the tuneout on the polarization of the input light. The intensity, purity, circularity, and principal axis of the polarization of light are specified by the Stokes parameters, which can also be considered as a point on the Poincare sphere. This is analogous to specifying a qubit state by its Bloch vector, but the Stokes vector need not be normalized. Here are plots of the tuneout wavelength with respect to two of the stokes parameters, but in fact our data constrain a function on the unit sphere. Fitting this function to our data lets us specify linear combinations of the scalar, tensor, and vector polarizabilities, fixing a value for the tuneout wavelength that...

You'll learn all about at Bryce's final seminar, when we're done with our analysis.

## Many-body physics I: Quantum Depletion

(fractional power comes from Thomas Fermi expression for chemical potential)
4:17 (19:17)

Moving away from the world of atomic physics, another major focus of cold atom research is the study of extensive coherent phenomena like superfluidity and superconductivity. The tantalizing prospect of engineered supermaterials could lead to a revolutionary reduction in device efficiency, so the uniquely adept manipulation and measurement of quantum gas experiments are an obvious drawcard. While all BECs are superfluids, but not all of a superfluid is condensate - the particle density in a liquid superfluid is some ten orderd of magnitude times greater than in our BECs, which are themselves about one ten thousanth of atmospheric density. Interactions between atoms are much more significant in liquid helium than in a BEC, but as we'll see here there are still visible signatures.

To make the physics of condensates more tractable, a common approximation is to replace the state vector with a complex field whose amplitude squared is the ground-state density, and whose phase characterizes the extensive coherence after crossing the BEC transition. This cannot be a complete picture of a condensate, because it implies a zero-entropy state which is forbidden by the second law of thermodynamics. In fact, a condensate always coexists with a population of thermal atoms. The BEC itself, like any interacting ensemble, has a spectrum of collective excitations. Even at the ground state, fluctuations in the quasiparticle vacuum result in the occupation of higher momentum modes than the ground state. The amplitude of these fluctuations scales with the dimensionless gas parameter which characterizes the importance of interactions - by increasing with atomic density and interparticle scattering length. This population of nonzero momentum modes is known as the quantum depletion.

This is another context where metastable Helium shines - here's a section of the momentum profile of a condensate, which clearly shows the central peak of the condensed fraction, the thermal distribution of the non-condensed fraction, and the asymptotic power-law scaling of momentum density as predicted by Bogoliubov theory. The sticking point, though, is a reasonable-sounding argument that goes:
1) the depleted population scales with density
2) During expansion the density decreases on a timescale much slower than the trap dynamics
3) Therefore, the depletion should vanish adiabatically in the free expansion.
And yet here it is, apparently as clear as day. This data came from another Helium group in France, and when we found this paper, we went downstairs to reproduce their results and had data rolling in by the end of the week.

Before we go over our findings, let's recap what IS known about the quantum depletion. Here are two graphs from past works showing great agreement between theory and measurement. On the left, the depth of an optical lattice potential mediates the gas parameter by forcing up the particle density. Absorption imaging retrieves a profile of the lattice interference pattern, plus an incoherent background. The depleted population is measured by integrating the background density, and agrees quite well with the curves predicted theoretically. On the right, the scattering length of a homogeneous BEC is tuned via Feshbach resonance, and the depletion fraction is obtained by momentum-selective Bragg spectroscopy, again in great agreement with predictions.

Turning back to the Parisian lab, their experiment resolved the momentum profile by dropping BECs straight onto their delay-line detector. They varied the density through control of the trapping frequency, and compute the depleted population by fitting the power-law region of the momentum profile. They claim a dependence which  is six times stronger than the prediction from the otherwise successful Bogolibov theory. 

In our reproduction of the experiment, we also resolve a depleted population. However, our dependence is much weaker. Last year, I passed through France and knocked on their door to ask what's up. I learned that since publishing their findings in PRL, they'd found that their BECs weren't perfectly spin-polarized. Because our trap is magnetic, by definition we can only trap one magnetic state of the metastable state. In contrast, they used an optical dipole trap, which can confine a mixture of spin states. After correcting this defect in their experiment, they'd found that the depleted population is still visible, but with a dependence only 1.7 times stronger than theory - right in the realm of our results. 

So there's consistend divergence of theory and experiment, which means there's more to be understood. This case isn't resolved yet. Our next step is to do some momentum-selective Bragg spectroscopy right after the trap switch-off, to try and determine the root of this discrepancy.

## Many-body physics II: Optical Lattices

3:47 (23:00)

Our final leap this afternoon is from the weakly to the strongly interacting regime, towards quantum state engineering and the state of the art of ultracold atom systems: An optical lattice for quantum simulation.

Broadly speaking, quantum physics is hard because only a handful of models are exactly solvable, and because the state space grows exponentially with the number of subsystems. Therefore, if we want to get a handle on large quantum states for applications in sensing, metamaterials, or computing, we need a way around this obstruction. The posterchild for solutions to this problem are quantum computers, but in the arms race for computational advantage, classical computers don't look like they're about to give up the ghost just yet. However, the capability of so-called quantum simulators, or controllable quantum systems that directly realize a problem of interest beyond the limits of modern classical simulation, is one way to harness a quantum advantage without a universal quantum computer. 

Optical lattices are one paradigm of quantum simulators, especially suited to problems in condensed matter, which bears closer and closer resemblance to high-energy physics if you go deep enough into the theoretical rabbit hole. In brief, optical lattices synthesize controllable potentials by creating standing waves with intersecting laser beams, corralling the atoms by virtue of the aforementioned dipole force. At high intensities these lattices force atoms to localize much like electrons bound in crystal lattices, but completely free of defects or coupling to lattice phonons. The state of the art has realized single-site readout, population, potential, and spin engineering. In addition to fine control over the vertex terms of engineered Hamiltionians, labs around the world have developed artificial gauge fields by creating virtual flux quanta through the plaquettes of the lattice, forcing neutral atoms to simulate charged particles in synthetic fields with strength exceeding those presently realizable with actual magnets. But in all of this, the poor old momentum operator is barely mentioned at all. Given that an entire research industry is build upon in-situ detection and control in optical lattices, the prospects of metastable helium for detailed study of lattice reciprocal space promises to usher in new insights into condensed matter phenomena.

The first stage of lattice development is the realization of the bose-hubbard model, a simple Hamiltonian that describes bosons interacting with some characteristic strength U, and tunnelling with some rate J, whose relative importance depends on the lattice depth as determined by the intensity of the lattice beams. At low depths, the tunnelling term dominates the hamiltonian, the atoms delocalize in a superfluid phase characterized by a gapless spectrum and long-range phase coherence. For strong lattices, tunnelling through between sites is suppressed and interactions dominate, creating a gapped spectrum in the so-called Mott insulating phase. This is the foundation on which further capabilities can be built. 

We're not there yet. In the meantime, here are some happy snaps from the building process - the time we played Lego with a couple hunded kilos of precision-machined steel in the assembly of our new vacuum chamber, the first iteration of the magnetic trap with big chunky magnetic coils around the optical insertion windows, and following the upgrade to in-vacuum coils with an external bias and super-sciencey bright green coolant lines. This is an absorption image taken of the cloud held in a magnetic trap, where brightness shows the density of the atomic cloud, ready for evaporative cooling. There's some optimization left before we have another BEC in the family, so... watch this space.

## Conclusion

0:40 (23:45)

As I said, we're a strongly collaborative group. Externally, Piotr Deuar has provided great support on the depletion project, Gordon Drake and Li-Yan Tan made our tuneout measurement possible, and Chris Vale lent us the laser we've been flogging for the last twelve months, even sending their postdoc Carlos along for some of the three months it took us to get the damn thing aligned. Thanks to our indispensable technicians Ross and Colin, we have an experiment in the first place, and thanks to the technical and moral support of my supervisors and fellow students, none of this would have happened. And, of course, thanks to all of you, my captive audience, for your attention. 
